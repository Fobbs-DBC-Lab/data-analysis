{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Test Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.2.3\n",
      "NumPy version: 2.0.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# This requires Python 3.7 and above\n",
    "print('Pandas version:', pd.__version__)\n",
    "print('NumPy version:', np.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV File Reader Function\n",
    "\n",
    "Reads multiple CSV files starting with \"FED\" from a directory and combines them into a single DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What it does\n",
    "- Scans directory for \"FED*.CSV\" files\n",
    "- Reads each file and adds a `file_num` identifier column\n",
    "- Combines all files into one DataFrame\n",
    "- Converts \"MM:DD:YYYY hh:mm:ss\" to datetime format\n",
    "\n",
    "#### Usage\n",
    "```python\n",
    "# Import and use\n",
    "from your_module import read_csv_files\n",
    "combined_data = read_csv_files(\"/path/to/csv/files\")\n",
    "\n",
    "# Work with results\n",
    "print(f\"Total records: {len(combined_data)}\")\n",
    "```\n",
    "\n",
    "#### Notes\n",
    "- Handles file reading errors gracefully\n",
    "- Returns empty DataFrame if no valid files found\n",
    "- Datetime sorting is available (commented out by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_files(file_path):\n",
    "    \"\"\"\n",
    "    Reads CSV files that start with 'FED' and organizes them into a single dataframe with file categories.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the directory containing CSV files\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A single dataframe containing data from all FED CSV files\n",
    "    \"\"\"\n",
    "    all_dataframes = []\n",
    "    file_counter = 1  # Initialize counter for file categories\n",
    "\n",
    "    # For all files in folder\n",
    "    for file in os.listdir(file_path):\n",
    "        if file.endswith(\".CSV\") and file.startswith(\"FED\"):\n",
    "            # Read that file into a dataframe\n",
    "            file_path_df = os.path.join(file_path, file)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path_df)\n",
    "                \n",
    "                # Add file category column\n",
    "                df[\"file_num\"] = file_counter\n",
    "                file_counter += 1\n",
    "                \n",
    "                all_dataframes.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file}: {e}\")\n",
    "    \n",
    "    if not all_dataframes:\n",
    "        print(\"No valid FED CSV files found in the specified directory.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # Create a single dataframe from all files\n",
    "    singular_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # Convert datetime column to datetime type\n",
    "    try:\n",
    "        singular_df[\"MM:DD:YYYY hh:mm:ss\"] = pd.to_datetime(singular_df[\"MM:DD:YYYY hh:mm:ss\"])\n",
    "        # Uncomment the line below to sort by datetime\n",
    "        # singular_df = singular_df.sort_values(by=['MM:DD:YYYY hh:mm:ss'], ascending=True)\n",
    "    except KeyError:\n",
    "        print(\"Warning: 'MM:DD:YYYY hh:mm:ss' column not found. Skipping datetime conversion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting datetime: {e}\")\n",
    "    \n",
    "    return singular_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data File Paths and File Counter in Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code defines file path variables for a data analysis project in the Fobbs Lab. \n",
    "- It specifies paths to directories containing mouse experimental data, particularly for the \"Chow Group\" and a specific mouse (M281) with a feeding device (FED004). \n",
    "- It also defines variables for the mouse ID, feeding device ID, and date (February 23, 2025). \n",
    "- Finally, it prints the number of files found in the individual mouse's directory by using the os.listdir() function to count files and displaying the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files found: \n",
      "39\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MICE_GROUP_PATH ='/Users/kevinmcpherson/github-projects/fobbs-lab/data-analysis/local_files/input/Chow Group/'\n",
    "INDIVIDUAL_MOUSE_PATH = '/Users/kevinmcpherson/github-projects/fobbs-lab/data-analysis/local_files/input/Chow Group/m281_FED004/'\n",
    "SD_ANALYSIS_FILES_PATH = '/Users/kevinmcpherson/github-projects/fobbs-lab/data-analysis/local_files/input/SD Analyses/'\n",
    "MOUSE = 'M281'\n",
    "FED = '_FED004'\n",
    "DATE = '022325'\n",
    "\n",
    "\n",
    "print (\"Number of files found: \")\n",
    "print(len(os.listdir(INDIVIDUAL_MOUSE_PATH)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing FED Device Data Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script processes FED (Feeding Experimentation Device) data files by adding two new columns that track the beginning and end dates for each file number in the dataset.\n",
    "\n",
    "### Input Data Structure\n",
    "The script expects a CSV file with the following key columns:\n",
    "- An unnamed index column (first column)\n",
    "- `MM:DD:YYYY hh:mm:ss`: Timestamp column in the format \"M/D/YYYY HH:MM:SS\"\n",
    "- `file_num`: Integer column indicating the file number\n",
    "\n",
    "### New Columns Added\n",
    "The script adds two new columns to the dataset:\n",
    "- `file_begin_date`: The date (MM/DD/YYYY) when each file_num first appears\n",
    "- `file_end_date`: The date (MM/DD/YYYY) when each file_num last appears\n",
    "\n",
    "### Processing Steps\n",
    "1. Reads the CSV file using pandas, setting the unnamed first column as the index\n",
    "2. Converts the \"MM:DD:YYYY hh:mm:ss\" column to datetime format for proper date handling\n",
    "3. Groups the data by file_num to identify:\n",
    "   - First timestamp for each file number (begin_date)\n",
    "   - Last timestamp for each file number (end_date)\n",
    "4. Creates new columns mapping these dates back to the original dataframe\n",
    "5. Formats dates in MM/DD/YYYY format\n",
    "6. Saves the processed data to a new CSV file\n",
    "\n",
    "### Usage\n",
    "```python\n",
    "input_file = \"SD Analyses M281 FED004.csv\"\n",
    "output_file = \"SD Analyses M281 FED004_processed.csv\"\n",
    "processed_df = process_fed_dates(input_file, output_file)\n",
    "```\n",
    "\n",
    "### Output\n",
    "The script creates a new CSV file with all original columns plus:\n",
    "- file_begin_date\n",
    "- file_end_date\n",
    "\n",
    "It also prints:\n",
    "- A sample of the processed data (first 5 rows)\n",
    "- A summary showing the date ranges for each file number\n",
    "\n",
    "Note: The original timestamp format is preserved in the \"MM:DD:YYYY hh:mm:ss\" column, while the new date columns are formatted as MM/DD/YYYY for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fed_dates(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Process FED device CSV file to add file_begin_date and file_end_date columns.\n",
    "\n",
    "    Parameters:\n",
    "    input_file (str): Path to input CSV file\n",
    "    output_file (str): Path to save the processed CSV file\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    # Note: the first column is unnamed and just an index, so we'll use it as the index\n",
    "    df = pd.read_csv(input_file, index_col=0)\n",
    "\n",
    "    # Convert datetime column to datetime type\n",
    "    df['MM:DD:YYYY hh:mm:ss'] = pd.to_datetime(df['MM:DD:YYYY hh:mm:ss'])\n",
    "\n",
    "    # Group by file_num and get first and last dates\n",
    "    file_dates = df.groupby('file_num').agg({\n",
    "        'MM:DD:YYYY hh:mm:ss': ['first', 'last']\n",
    "    })\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    file_dates.columns = ['begin_date', 'end_date']\n",
    "\n",
    "    # Convert to dictionary for mapping\n",
    "    begin_dates = file_dates['begin_date'].to_dict()\n",
    "    end_dates = file_dates['end_date'].to_dict()\n",
    "\n",
    "    # Add new columns to original dataframe\n",
    "    df['file_begin_date'] = df['file_num'].map(begin_dates).dt.strftime('%m/%d/%Y')\n",
    "    df['file_end_date'] = df['file_num'].map(end_dates).dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    # Save to new CSV file\n",
    "    df.to_csv(output_file)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Reader Function Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function processes multiple CSV files from a specified directory and combines them into a single pandas DataFrame. Here's what it does:\n",
    "\n",
    "1. **Filters Files**: It only processes files that:\n",
    "   - Have a `.CSV` extension\n",
    "   - Start with `FED` in their filename\n",
    "\n",
    "2. **Adds Metadata**: Each file's data is tagged with a sequential number (`file_num`) to track which file the data came from.\n",
    "\n",
    "3. **Combines Data**: All individual DataFrames are concatenated into a single DataFrame.\n",
    "\n",
    "4. **Date Processing**: It converts a date-time column named `MM:DD:YYYY hh:mm:ss` to pandas datetime format for better date handling.\n",
    "\n",
    "5. **Optional Sorting**: Contains a commented-out line that would sort the data by the date-time column if uncommented.\n",
    "\n",
    "This function is useful for analyzing data spread across multiple CSV files that follow a similar format, particularly for time series data from multiple collection periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_files(file_path):\n",
    "    \"\"\"\n",
    "    Reads CSV files that start with 'FED' from a directory and combines them \n",
    "    into a single dataframe with file numbering.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the directory containing CSV files\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Combined dataframe with all CSV data and file numbers\n",
    "    \"\"\"\n",
    "    all_dataframes = []\n",
    "    file_counter = 1\n",
    "    \n",
    "    # Iterate through all files in the specified directory\n",
    "    for file in os.listdir(file_path):\n",
    "        # Process only CSV files that start with \"FED\"\n",
    "        if file.endswith(\".CSV\") and file.startswith(\"FED\"):\n",
    "            # Construct the full file path\n",
    "            file_path_df = os.path.join(file_path, file)\n",
    "            \n",
    "            # Read the CSV into a dataframe\n",
    "            df = pd.read_csv(file_path_df)\n",
    "            \n",
    "            # Add a file number identifier column\n",
    "            df['file_num'] = file_counter\n",
    "            file_counter += 1\n",
    "            \n",
    "            # Add to our collection\n",
    "            all_dataframes.append(df)\n",
    "    \n",
    "    # Exit early if no files were found\n",
    "    if not all_dataframes:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # Combine all dataframes into one\n",
    "    singular_df = pd.concat(all_dataframes)\n",
    "    \n",
    "    # Convert date-time column to datetime type\n",
    "    singular_df['MM:DD:YYYY hh:mm:ss'] = pd.to_datetime(singular_df['MM:DD:YYYY hh:mm:ss'])\n",
    "    \n",
    "    # Uncomment the below line to sort by date-time\n",
    "    # singular_df = singular_df.sort_values(by=['MM:DD:YYYY hh:mm:ss'], ascending=True)\n",
    "    \n",
    "    return singular_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make concatentated file for `input_file` field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MM:DD:YYYY hh:mm:ss</th>\n",
       "      <th>Library_Version</th>\n",
       "      <th>Session_type</th>\n",
       "      <th>Device_Number</th>\n",
       "      <th>Battery_Voltage</th>\n",
       "      <th>Motor_Turns</th>\n",
       "      <th>FR</th>\n",
       "      <th>Event</th>\n",
       "      <th>Active_Poke</th>\n",
       "      <th>Left_Poke_Count</th>\n",
       "      <th>Right_Poke_Count</th>\n",
       "      <th>Pellet_Count</th>\n",
       "      <th>Block_Pellet_Count</th>\n",
       "      <th>Retrieval_Time</th>\n",
       "      <th>InterPelletInterval</th>\n",
       "      <th>Poke_Time</th>\n",
       "      <th>file_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-04-10 12:44:25</td>\n",
       "      <td>1.14.0</td>\n",
       "      <td>ClosedEcon</td>\n",
       "      <td>4</td>\n",
       "      <td>3.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Left</td>\n",
       "      <td>Left</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-04-10 12:44:33</td>\n",
       "      <td>1.14.0</td>\n",
       "      <td>ClosedEcon</td>\n",
       "      <td>4</td>\n",
       "      <td>3.83</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Pellet</td>\n",
       "      <td>Left</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-04-10 12:45:53</td>\n",
       "      <td>1.14.0</td>\n",
       "      <td>ClosedEcon</td>\n",
       "      <td>4</td>\n",
       "      <td>3.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Left</td>\n",
       "      <td>Left</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-10 12:45:57</td>\n",
       "      <td>1.14.0</td>\n",
       "      <td>ClosedEcon</td>\n",
       "      <td>4</td>\n",
       "      <td>3.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Left</td>\n",
       "      <td>Left</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-04-10 12:46:00</td>\n",
       "      <td>1.14.0</td>\n",
       "      <td>ClosedEcon</td>\n",
       "      <td>4</td>\n",
       "      <td>3.83</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Pellet</td>\n",
       "      <td>Left</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.53</td>\n",
       "      <td>88.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-04-10 12:47:24</td>\n",
       "      <td>1.14.0</td>\n",
       "      <td>ClosedEcon</td>\n",
       "      <td>4</td>\n",
       "      <td>3.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Left</td>\n",
       "      <td>Left</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-04-10 12:50:32</td>\n",
       "      <td>1.14.0</td>\n",
       "      <td>ClosedEcon</td>\n",
       "      <td>4</td>\n",
       "      <td>3.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Left</td>\n",
       "      <td>Left</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-04-10 12:50:34</td>\n",
       "      <td>1.14.0</td>\n",
       "      <td>ClosedEcon</td>\n",
       "      <td>4</td>\n",
       "      <td>3.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Left</td>\n",
       "      <td>Left</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-04-10 12:50:35</td>\n",
       "      <td>1.14.0</td>\n",
       "      <td>ClosedEcon</td>\n",
       "      <td>4</td>\n",
       "      <td>3.83</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Pellet</td>\n",
       "      <td>Left</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.42</td>\n",
       "      <td>275.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-04-10 12:54:14</td>\n",
       "      <td>1.14.0</td>\n",
       "      <td>ClosedEcon</td>\n",
       "      <td>4</td>\n",
       "      <td>3.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>Right</td>\n",
       "      <td>Left</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  MM:DD:YYYY hh:mm:ss Library_Version Session_type  Device_Number  \\\n",
       "0 2024-04-10 12:44:25          1.14.0   ClosedEcon              4   \n",
       "1 2024-04-10 12:44:33          1.14.0   ClosedEcon              4   \n",
       "2 2024-04-10 12:45:53          1.14.0   ClosedEcon              4   \n",
       "3 2024-04-10 12:45:57          1.14.0   ClosedEcon              4   \n",
       "4 2024-04-10 12:46:00          1.14.0   ClosedEcon              4   \n",
       "5 2024-04-10 12:47:24          1.14.0   ClosedEcon              4   \n",
       "6 2024-04-10 12:50:32          1.14.0   ClosedEcon              4   \n",
       "7 2024-04-10 12:50:34          1.14.0   ClosedEcon              4   \n",
       "8 2024-04-10 12:50:35          1.14.0   ClosedEcon              4   \n",
       "9 2024-04-10 12:54:14          1.14.0   ClosedEcon              4   \n",
       "\n",
       "   Battery_Voltage  Motor_Turns  FR   Event Active_Poke  Left_Poke_Count  \\\n",
       "0             3.83          NaN   1    Left        Left                1   \n",
       "1             3.83          3.0   1  Pellet        Left                1   \n",
       "2             3.83          NaN   2    Left        Left                2   \n",
       "3             3.83          NaN   2    Left        Left                3   \n",
       "4             3.83          1.0   2  Pellet        Left                3   \n",
       "5             3.83          NaN   3    Left        Left                4   \n",
       "6             3.83          NaN   3    Left        Left                5   \n",
       "7             3.83          NaN   3    Left        Left                6   \n",
       "8             3.83          1.0   3  Pellet        Left                6   \n",
       "9             3.83          NaN   4   Right        Left                6   \n",
       "\n",
       "   Right_Poke_Count  Pellet_Count  Block_Pellet_Count Retrieval_Time  \\\n",
       "0                 0             0                   0            NaN   \n",
       "1                 0             1                   1           1.89   \n",
       "2                 0             1                   1            NaN   \n",
       "3                 0             1                   1            NaN   \n",
       "4                 0             2                   2           1.53   \n",
       "5                 0             2                   2            NaN   \n",
       "6                 0             2                   2            NaN   \n",
       "7                 0             2                   2            NaN   \n",
       "8                 0             3                   3           0.42   \n",
       "9                 1             3                   3            NaN   \n",
       "\n",
       "   InterPelletInterval  Poke_Time  file_num  \n",
       "0                  NaN       0.22         1  \n",
       "1                  NaN        NaN         1  \n",
       "2                  NaN       0.17         1  \n",
       "3                  NaN       0.05         1  \n",
       "4                 88.0        NaN         1  \n",
       "5                  NaN       0.11         1  \n",
       "6                  NaN       0.21         1  \n",
       "7                  NaN       0.13         1  \n",
       "8                275.0        NaN         1  \n",
       "9                  NaN       0.05         1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and concatenate all FED CSV files from the individual mouse directory\n",
    "# INDIVIDUAL_MOUSE_PATH should contain the specific FED folder for the mouse being analyzed\n",
    "concatenated_dataframe = read_csv_files(INDIVIDUAL_MOUSE_PATH)\n",
    "\n",
    "# Define the specific output path for saving the concatenated data\n",
    "# This will save the file in the specified directory with a formatted filename\n",
    "output_path = \"/Users/kevinmcpherson/github-projects/fobbs-lab/data-analysis/local_files/output\"\n",
    "\n",
    "# Construct the full output filename with mouse ID, FED device number, and date\n",
    "output_filename = f\"{MOUSE}_{FED}_concat1_{DATE}.csv\"\n",
    "\n",
    "# Combine the path and filename for the complete file location\n",
    "output_file_path = os.path.join(output_path, output_filename)\n",
    "\n",
    "# Save the concatenated data to the specified file path\n",
    "concatenated_dataframe.to_csv(output_file_path)\n",
    "\n",
    "# Display the first 10 rows of the concatenated dataframe for verification\n",
    "concatenated_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. New file saved as: /Users/kevinmcpherson/github-projects/fobbs-lab/data-analysis/local_files/output/M281__FED004_concat2_022325.csv\n",
      "\n",
      "Sample of processed data (first 5 rows):\n",
      "   file_num MM:DD:YYYY hh:mm:ss file_begin_date file_end_date\n",
      "0         1 2024-04-10 12:44:25      04/10/2024    04/11/2024\n",
      "1         1 2024-04-10 12:44:33      04/10/2024    04/11/2024\n",
      "2         1 2024-04-10 12:45:53      04/10/2024    04/11/2024\n",
      "3         1 2024-04-10 12:45:57      04/10/2024    04/11/2024\n",
      "4         1 2024-04-10 12:46:00      04/10/2024    04/11/2024\n",
      "\n",
      "Summary of file number date ranges:\n",
      "         file_begin_date file_end_date\n",
      "file_num                              \n",
      "1             04/10/2024    04/11/2024\n",
      "2             04/11/2024    04/12/2024\n",
      "3             03/28/2024    03/29/2024\n",
      "4             03/29/2024    03/30/2024\n",
      "5             04/24/2024    04/25/2024\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_file = '/Users/kevinmcpherson/github-projects/fobbs-lab/data-analysis/local_files/output/M281__FED004_concat1_022325.csv'\n",
    "    output_file = '/Users/kevinmcpherson/github-projects/fobbs-lab/data-analysis/local_files/output/M281__FED004_concat2_022325.csv'\n",
    "\n",
    "    processed_df = process_fed_dates(input_file, output_file)\n",
    "    print(\"Processing complete. New file saved as:\", output_file)\n",
    "\n",
    "    # Display sample of processed data\n",
    "    print(\"\\nSample of processed data (first 5 rows):\")\n",
    "    print(processed_df[['file_num', 'MM:DD:YYYY hh:mm:ss', 'file_begin_date', 'file_end_date']].head())\n",
    "\n",
    "    # Print summary of file numbers and their date ranges\n",
    "    summary = processed_df.groupby('file_num').agg({\n",
    "        'file_begin_date': 'first',\n",
    "        'file_end_date': 'first'\n",
    "    })\n",
    "    print(\"\\nSummary of file number date ranges:\")\n",
    "    print(summary.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FR Reset Counter: How It Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "The FR Reset Counter tracks when a fixed ratio (FR) schedule resets from a higher value back to 1 in behavioral data. This is particularly useful for analyzing progressive ratio or other variable schedule experiments.\n",
    "\n",
    "#### Step-by-Step Process\n",
    "\n",
    "1. **Data Preparation**\n",
    "   - The function creates a working copy of the data to avoid modifying the original\n",
    "   - Converts the FR column to numeric values, handling any non-numeric entries\n",
    "   - Initializes a new column `FR_Reset_Count` with zeros\n",
    "\n",
    "2. **File-by-File Processing**\n",
    "   - Data is processed independently for each `file_num` (typically representing different sessions)\n",
    "   - For each file:\n",
    "     - Rows are sorted by timestamp to ensure chronological order\n",
    "     - A `prev_FR` column is created by shifting the FR values to compare each row with its predecessor\n",
    "\n",
    "3. **Reset Detection**\n",
    "   - A reset is identified when:\n",
    "     - Current FR value is exactly 1, AND\n",
    "     - Previous FR value was greater than 1\n",
    "   - This creates a boolean mask of where resets occur\n",
    "\n",
    "4. **Cumulative Counting**\n",
    "   - The running total of resets is maintained using `cumsum()` on the boolean mask\n",
    "   - This creates a continuous counter that increments with each reset\n",
    "\n",
    "5. **Results Integration**\n",
    "   - The reset counts are merged back into the original dataframe\n",
    "   - The counter resets to 0 for each new file_num\n",
    "\n",
    "#### Handling Edge Cases\n",
    "- Initial FR=1 values are not counted as resets (no previous higher value)\n",
    "- NaN values in FR are properly handled\n",
    "- Each file_num maintains its own independent counter\n",
    "\n",
    "#### Example\n",
    "When FR progresses 1→2→3→4→1→2→3, the reset counter will increment to 1 at the point where FR changes from 4 back to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_fr_resets(data):\n",
    "    \"\"\"\n",
    "    Count the number of times FR resets to 1 from a higher value for each unique file_num.\n",
    "    \n",
    "    Parameters:\n",
    "    data (DataFrame): DataFrame containing the behavioral data with columns 'file_num' and 'FR'\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Original DataFrame with a new column 'FR_Reset_Count' that shows a cumulative\n",
    "              count of FR resets within each file_num\n",
    "    \"\"\"\n",
    "    # Make a copy of the input data to avoid modifying the original\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Convert FR column to numeric if it's not already\n",
    "    df['FR'] = pd.to_numeric(df['FR'], errors='coerce')\n",
    "    \n",
    "    # Initialize the FR_Reset_Count column with zeros\n",
    "    df['FR_Reset_Count'] = 0\n",
    "    \n",
    "    # Process each file_num separately\n",
    "    for file_num in df['file_num'].unique():\n",
    "        # Get the subset for this file\n",
    "        file_mask = df['file_num'] == file_num\n",
    "        file_data = df.loc[file_mask].copy()\n",
    "        \n",
    "        # Sort by the timestamp to ensure correct order\n",
    "        file_data = file_data.sort_values('MM:DD:YYYY hh:mm:ss')\n",
    "        \n",
    "        # Find resets where FR changes from >1 to 1\n",
    "        # Shift to compare with previous row\n",
    "        file_data['prev_FR'] = file_data['FR'].shift(1)\n",
    "        \n",
    "        # Mark rows where FR is 1 and previous FR was >1\n",
    "        reset_mask = (file_data['FR'] == 1) & (file_data['prev_FR'] > 1)\n",
    "        \n",
    "        # Count resets cumulatively\n",
    "        file_data['FR_Reset_Count'] = reset_mask.cumsum()\n",
    "        \n",
    "        # Update the main dataframe with the reset counts\n",
    "        df.loc[file_mask, 'FR_Reset_Count'] = file_data['FR_Reset_Count']\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.read_csv('your_data.csv', sep='\\t')\n",
    "# result = count_fr_resets(df)\n",
    "# print(result[['file_num', 'FR', 'FR_Reset_Count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edit3 = count_fr_resets(processed_df)\n",
    "df_edit3.to_csv('/Users/kevinmcpherson/github-projects/fobbs-lab/data-analysis/local_files/output/M281__FED004_concat3_022325.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meal Counter and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_meals(data):\n",
    "    \"\"\"\n",
    "    Count meals based on pellet events associated with each FR reset.\n",
    "    Reset meal count to 1 when a new day starts with significant time passing.\n",
    "    \n",
    "    Parameters:\n",
    "    data (DataFrame): DataFrame containing the behavioral data with columns 'file_num', \n",
    "                     'FR_Reset_Count', 'Event', and 'MM:DD:YYYY hh:mm:ss'. The data should \n",
    "                     already have FR_Reset_Count calculated by the count_fr_resets function.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Original DataFrame with one new column:\n",
    "              - 'Meal_Number': Identifies which meal the row belongs to\n",
    "    \"\"\"\n",
    "    # Make a copy of the input data to avoid modifying the original\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Initialize new column\n",
    "    df['Meal_Number'] = 0\n",
    "    \n",
    "    # Process each file_num separately\n",
    "    for file_num in df['file_num'].unique():\n",
    "        # Get the subset for this file\n",
    "        file_mask = df['file_num'] == file_num\n",
    "        file_data = df.loc[file_mask].copy()\n",
    "        \n",
    "        # Sort by the timestamp to ensure correct order\n",
    "        file_data = file_data.sort_values('MM:DD:YYYY hh:mm:ss')\n",
    "        \n",
    "        # Convert timestamp to datetime for day comparison\n",
    "        file_data['datetime'] = pd.to_datetime(file_data['MM:DD:YYYY hh:mm:ss'])\n",
    "        \n",
    "        # Create a meal identifier combining file_num and FR_Reset_Count\n",
    "        file_data['meal_id'] = file_data['file_num'].astype(str) + '_' + file_data['FR_Reset_Count'].astype(str)\n",
    "        \n",
    "        # Track unique meal IDs and whether they've had a pellet event\n",
    "        meal_pellets = {}\n",
    "        meal_counters = {}\n",
    "        current_meal_count = 0\n",
    "        \n",
    "        # Track the current day and last timestamp for day change detection\n",
    "        current_day = None\n",
    "        last_timestamp = None\n",
    "        \n",
    "        # Process rows sequentially\n",
    "        for idx, row in file_data.iterrows():\n",
    "            meal_id = row['meal_id']\n",
    "            current_timestamp = row['datetime']\n",
    "            \n",
    "            # Extract the current day\n",
    "            row_day = current_timestamp.date()\n",
    "            \n",
    "            # Check if a new day has started with significant time passing\n",
    "            if current_day is not None and row_day != current_day:\n",
    "                # Reset meal count to 1 for the new day\n",
    "                # Only reset if significant time has passed (e.g., 4 hours)\n",
    "                time_difference = (current_timestamp - last_timestamp).total_seconds() / 3600  # hours\n",
    "                if time_difference >= 4:  # Significant time (4 hours) threshold\n",
    "                    current_meal_count = 0  # Will become 1 when we encounter a pellet\n",
    "                    # Reset all meal pellets tracking for the new day\n",
    "                    meal_pellets = {}\n",
    "            \n",
    "            # Update current day and timestamp\n",
    "            current_day = row_day\n",
    "            last_timestamp = current_timestamp\n",
    "            \n",
    "            # Check if this is a pellet event\n",
    "            is_pellet = row['Event'] == 'Pellet'\n",
    "            \n",
    "            # If we haven't seen this meal ID before, initialize it\n",
    "            if meal_id not in meal_pellets:\n",
    "                meal_pellets[meal_id] = False\n",
    "                \n",
    "            # If this is a pellet event and we haven't counted a pellet for this meal yet\n",
    "            if is_pellet and not meal_pellets[meal_id]:\n",
    "                meal_pellets[meal_id] = True\n",
    "                current_meal_count += 1\n",
    "                meal_counters[meal_id] = current_meal_count\n",
    "            \n",
    "            # Assign meal number (0 if no pellet for this meal yet)\n",
    "            if meal_pellets[meal_id]:\n",
    "                file_data.at[idx, 'Meal_Number'] = meal_counters[meal_id]\n",
    "        \n",
    "        # Update the main dataframe with the meal numbers\n",
    "        df.loc[file_mask, 'Meal_Number'] = file_data['Meal_Number']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# First apply the FR reset counting function\n",
    "# df_with_resets = count_fr_resets(df)\n",
    "# Then count meals based on these resets\n",
    "# result = count_meals(df_with_resets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edit4 = count_meals(df_edit3)\n",
    "df_edit4.to_csv('/Users/kevinmcpherson/github-projects/fobbs-lab/data-analysis/local_files/output/M281__FED004_concat4_022325.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
